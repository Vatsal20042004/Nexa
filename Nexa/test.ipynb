{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b3bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_ = \"\"\"```json\n",
    "[\n",
    "  {\n",
    "    \"type\": \"meeting_transcript\",\n",
    "    \"timestamp\": \"2025-11-07T18:00:00Z\",\n",
    "    \"data\": {\n",
    "      \"purpose\": \"Evening Review\",\n",
    "      \"content\": \"Team Lead: Alright, team, let's wrap up for the day. Starting with jdoe - what did you accomplish today, any blockers, and what's carrying over?\\njdoe: Today, I focused on the backend scaling for the search API. I completed the initial load testing using JMeter, simulating 10k concurrent users, and identified bottlenecks in the query optimization. I pushed commits to the feat/api-scaling branch with optimizations like index additions and query caching hints. However, I didn't finish integrating the new Redis caching layer because I ran into configuration issues with the cluster setup in our GCP environment. Also, the database migration script for the users schema is still pending - I have the script ready, but there are schema conflicts with the existing prod data that need resolution. I spent about 2 hours debugging that but couldn't resolve it without input from the DBA team. So, those two are carrying over to tomorrow.\\nTeam Lead: Noted. @bsmith, can you review jdoe's migration script first thing tomorrow?\\n@bsmith: Yes, I'll prioritize that.\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"github_action\",\n",
    "    \"timestamp\": \"2025-11-07T20:45:00Z\",\n",
    "    \"data\": {\n",
    "      \"user\": \"jdoe\",\n",
    "      \"action\": \"commit\",\n",
    "      \"branch\": \"feat/api-scaling\",\n",
    "      \"message\": \"add: preliminary scaling optimizations\\n\\n- Added composite indexes on frequently queried fields in the search table.\\n- Implemented connection pooling for database queries to reduce overhead.\\n- Updated config for higher thread counts in the API server.\\n\\nTested locally with 5k simulated requests; response time improved by 40%.\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"github_action\",\n",
    "    \"timestamp\": \"2025-11-07T21:15:00Z\",\n",
    "    \"data\": {\n",
    "      \"user\": \"jdoe\",\n",
    "      \"action\": \"pr_opened\",\n",
    "      \"pr_id\": 789,\n",
    "      \"title\": \"Feat: API Scaling Optimizations\",\n",
    "      \"description\": \"This PR introduces initial scaling improvements for the backend search API. Includes indexing, pooling, and config tweaks. Awaiting review before merge. Related to issue #123.\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"meeting_transcript\",\n",
    "    \"timestamp\": \"2025-11-08T09:30:00Z\",\n",
    "    \"data\": {\n",
    "      \"purpose\": \"Morning Stand-up\",\n",
    "      \"content\": \"Team Lead: Good morning, team. Quick stand-up: Yesterday, today, blockers. jdoe, go ahead.\\njdoe: Yesterday, as mentioned in the evening review, I completed the scaling tests and optimizations but left the Redis integration and DB migration unfinished. Today, my top priority is finishing the caching integration with Redis - I estimate about 4 hours for that, including testing in staging. After that, I'll tackle the database migration, which is blocked pending @bsmith's review on the schema conflicts. Also, I saw the new security audit doc; the CVE-2025-1234 patch for our auth endpoints is urgent, so I'll slot that in as high priority once caching is done. Additionally, I got assigned issue #456 for OAuth refresh token handling - that's due EOD, so medium priority after the patch.\\nTeam Lead: Sounds good. @bsmith, get that review done ASAP.\\n@bsmith: On it, should be done by 10 AM.\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"ocr_capture\",\n",
    "    \"timestamp\": \"2025-11-08T10:15:00Z\",\n",
    "    \"data\": {\n",
    "      \"window_title\": \"VS Code - [google-backend/api-cache.js]\",\n",
    "      \"content\": \"// api-cache.js\\nimport Redis from 'ioredis';\\nconst redisClient = new Redis({ host: 'redis-cluster.google.internal', port: 6379 });\\n\\nfunction setupCacheLayer(client) {\\n  // Set up TTL for cache entries\\n  const TTL = 3600; // 1 hour\\n  client.on('error', (err) => console.error('Redis Client Error', err));\\n\\n  async function cacheQuery(key, queryFn) {\\n    const cached = await redisClient.get(key);\\n    if (cached) return JSON.parse(cached);\\n    const result = await queryFn();\\n    await redisClient.set(key, JSON.stringify(result), 'EX', TTL);\\n    return result;\\n  }\\n  // TODO: Handle cache invalidation on data updates\\n  return { cacheQuery };\\n}\\n\\nexport default setupCacheLayer;\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"github_action\",\n",
    "    \"timestamp\": \"2025-11-08T11:00:00Z\",\n",
    "    \"data\": {\n",
    "      \"user\": \"team-lead\",\n",
    "      \"action\": \"issue_assigned\",\n",
    "      \"issue_id\": 456,\n",
    "      \"title\": \"Implement OAuth refresh token handling\",\n",
    "      \"description\": \"Backend needs to support automatic refresh of OAuth tokens for long-lived sessions. Include error handling for revoked tokens. Assigned to jdoe; priority medium, due EOD today. Steps:\\n1. Update auth middleware to check token expiry.\\n2. Integrate refresh endpoint with Google OAuth API.\\n3. Add unit tests for refresh flow.\\n4. Deploy to staging for verification.\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"doc_scrape\",\n",
    "    \"timestamp\": \"2025-11-08T12:00:00Z\",\n",
    "    \"data\": {\n",
    "      \"source\": \"security-audit-q4.pdf\",\n",
    "      \"content\": \"Security Audit Report - Q4 2025\\n\\nExecutive Summary: Critical vulnerabilities identified in backend services.\\n\\nURGENT ACTION REQUIRED:\\n- CVE-2025-1234: Vulnerability in auth library allowing token replay attacks. Patch by updating to version 5.2.1 and enforcing nonce checks.\\n- Apply to all endpoints immediately; test in prod-like environment.\\n- Responsible: Backend Team (jdoe lead on auth).\\n\\nAdditional Recommendations:\\n- Enable full logging for auth events.\\n- Schedule penetration test post-patch.\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"ocr_capture\",\n",
    "    \"timestamp\": \"2025-11-08T14:30:00Z\",\n",
    "    \"data\": {\n",
    "      \"window_title\": \"Terminal - gcloud db migrate\",\n",
    "      \"content\": \"$ gcloud sql instances describe backend-db\\n...\\n$ ./migrate.sh --schema users\\nRunning migration...\\nApplying change: ALTER TABLE users ADD COLUMN oauth_refresh_token VARCHAR(255);\\nError: Schema conflict on users table - duplicate column detected from partial migration last week.\\nSuggestion: Run with --force to override, but risk data loss.\\nAborting migration.\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"github_action\",\n",
    "    \"timestamp\": \"2025-11-08T15:00:00Z\",\n",
    "    \"data\": {\n",
    "      \"user\": \"bsmith\",\n",
    "      \"action\": \"pr_review\",\n",
    "      \"pr_id\": 789,\n",
    "      \"comment\": \"Reviewed: Looks good, but add more comments on the pooling config. Approved with changes.\"\n",
    "    }\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**Target Employee Username:** jdoe (Google Backend Engineer handling intense workload in API scaling, security patching, and auth integrations, with personalization from yesterday's carry-over tasks like incomplete Redis caching and DB migration conflicts).\n",
    "\n",
    "**User Question to Ask the AI:** Analyze the detailed data for jdoe from yesterday and today, including full meeting transcripts, GitHub actions with commit messages and descriptions, OCR captures of code and terminal sessions, and document scrapes. Generate a prioritized daily task plan emphasizing backend optimizations, urgent security patches, OAuth handling, and resolving migration blockers under high-pressure deadlines, ensuring carry-over tasks from yesterday are addressed with detailed status updates resembling a real engineer's workflow.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d9c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.services import UnifiedService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1fd2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = UnifiedService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd4c19e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raja/Desktop/Nexa/env/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "res = ser.run_agentic_query(context=emp_ , question=\"create task and prioritise tasks for today along, with the time to complete in b/w 9 to 5\" , api_key=\"AIzaSyCN7_xw5_1AXxYU90oXfrsWC7HxIam-hMM\" , model_name=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92165680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Task(task='Finish caching integration with Redis', priority='High', etr='4 hours', status_comments='Status: In Progress (per OCR). Explicitly stated as top priority.'),\n",
       " Task(task='Apply CVE-2025-1234 patch for auth endpoints', priority='High', etr='Not Stated or Blocked - Time Pending', status_comments='Status: Planned. New URGENT requirement from security audit document and high priority per stand-up, to be actioned post-caching.'),\n",
       " Task(task='Implement OAuth refresh token handling (Issue #456)', priority='Medium', etr='Not Stated or Blocked - Time Pending', status_comments='Status: Planned. Assigned via GitHub and due EOD, to be actioned post-security patch.'),\n",
       " Task(task='Resolve database migration schema conflicts and apply script', priority='Low', etr='Not Stated or Blocked - Time Pending', status_comments='Status: Blocked by schema conflict (per OCR). Requires DBA input for resolution, carried over from yesterday.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f8ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
