[
    {
        "repository": "Rajasimhareddybolla/nexa",
        "sha": "7039511012d29f778f46f78c280ebcd975390ccd",
        "message": "added services ,lazy loading",
        "author": {
            "name": "raja",
            "email": "rajasimhabolla@gmail.com",
            "date": "2025-11-08T05:54:42Z"
        },
        "committer": {
            "name": "raja",
            "email": "rajasimhabolla@gmail.com",
            "date": "2025-11-08T05:54:42Z"
        },
        "stats": {
            "total": 229,
            "additions": 151,
            "deletions": 78
        },
        "files": [
            {
                "sha": "6903b1de6279c789f8aa945193fb2dd65b66da6e",
                "filename": "llm/__pycache__/agent_logic.cpython-310.pyc",
                "status": "removed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/Rajasimhareddybolla/Nexa/blob/60ade733e9c40f37bd51970fdf089880dc3b6525/llm%2F__pycache__%2Fagent_logic.cpython-310.pyc",
                "raw_url": "https://github.com/Rajasimhareddybolla/Nexa/raw/60ade733e9c40f37bd51970fdf089880dc3b6525/llm%2F__pycache__%2Fagent_logic.cpython-310.pyc",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/Nexa/contents/llm%2F__pycache__%2Fagent_logic.cpython-310.pyc?ref=60ade733e9c40f37bd51970fdf089880dc3b6525"
            },
            {
                "sha": "0e58b8880ae09436490d15405816e5b24c5cbeba",
                "filename": "requirements.txt",
                "status": "modified",
                "additions": 4,
                "deletions": 1,
                "changes": 5,
                "blob_url": "https://github.com/Rajasimhareddybolla/Nexa/blob/7039511012d29f778f46f78c280ebcd975390ccd/requirements.txt",
                "raw_url": "https://github.com/Rajasimhareddybolla/Nexa/raw/7039511012d29f778f46f78c280ebcd975390ccd/requirements.txt",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/Nexa/contents/requirements.txt?ref=7039511012d29f778f46f78c280ebcd975390ccd",
                "patch": "@@ -8,4 +8,7 @@ python-docx==0.8.11      # docx reading\n pandas==2.2.2            # csv reading if needed (lightweight usage)\n PyYAML==6.0              # yaml parsing\n markdown==3.4.4          # optional parsing of md to text\n-tomli==2.0.1 \n\\ No newline at end of file\n+tomli==2.0.1 \n+pyautogui\n+easyocr\n+Pillow\n\\ No newline at end of file"
            },
            {
                "sha": "1b80874e9e7fe7c3bf49eb9376132e6f4a16b557",
                "filename": "services/llm/agent_logic.py",
                "status": "renamed",
                "additions": 1,
                "deletions": 1,
                "changes": 2,
                "blob_url": "https://github.com/Rajasimhareddybolla/Nexa/blob/7039511012d29f778f46f78c280ebcd975390ccd/services%2Fllm%2Fagent_logic.py",
                "raw_url": "https://github.com/Rajasimhareddybolla/Nexa/raw/7039511012d29f778f46f78c280ebcd975390ccd/services%2Fllm%2Fagent_logic.py",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/Nexa/contents/services%2Fllm%2Fagent_logic.py?ref=7039511012d29f778f46f78c280ebcd975390ccd",
                "patch": "@@ -55,7 +55,7 @@ def get_query_generator_chain(model_name: str, base_url: Optional[str] = None, a\n \n     parser = PydanticOutputParser(pydantic_object=OutputFormat)\n \n-    system_prompt_1 = get_prompt(\"llm/assets/system_instructions.md\")\n+    system_prompt_1 = get_prompt(\"services/llm/assets/system_instructions.md\")\n \n     format_instructions = parser.get_format_instructions()\n     # Escape curly braces in format_instructions",
                "previous_filename": "llm/agent_logic.py"
            },
            {
                "sha": "b5bd094d16d43cc4ea4b03389bab3915cc7e35bd",
                "filename": "services/llm/assets/system_instructions.md",
                "status": "renamed",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/Rajasimhareddybolla/Nexa/blob/7039511012d29f778f46f78c280ebcd975390ccd/services%2Fllm%2Fassets%2Fsystem_instructions.md",
                "raw_url": "https://github.com/Rajasimhareddybolla/Nexa/raw/7039511012d29f778f46f78c280ebcd975390ccd/services%2Fllm%2Fassets%2Fsystem_instructions.md",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/Nexa/contents/services%2Fllm%2Fassets%2Fsystem_instructions.md?ref=7039511012d29f778f46f78c280ebcd975390ccd",
                "previous_filename": "llm/assets/system_instructions.md"
            },
            {
                "sha": "d6b199503544c451258f9d6bd468b36428fa1fd8",
                "filename": "services/services.py",
                "status": "modified",
                "additions": 124,
                "deletions": 37,
                "changes": 161,
                "blob_url": "https://github.com/Rajasimhareddybolla/Nexa/blob/7039511012d29f778f46f78c280ebcd975390ccd/services%2Fservices.py",
                "raw_url": "https://github.com/Rajasimhareddybolla/Nexa/raw/7039511012d29f778f46f78c280ebcd975390ccd/services%2Fservices.py",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/Nexa/contents/services%2Fservices.py?ref=7039511012d29f778f46f78c280ebcd975390ccd",
                "patch": "@@ -4,6 +4,7 @@\n import os\n from datetime import datetime\n from typing import Optional, Dict, Any\n+import logging\n \n # Import Universal Extractor classes\n from .extractors.pdf_extractor import PDFExtractor\n@@ -15,13 +16,10 @@\n from .extractors.toml_extractor import TOMLExtractor\n from .extractors.markdown_extractor import MarkdownExtractor\n \n-# Import Nexy-Rep functionalities\n-from .nexy_rep.capture import take_screenshot\n-from .nexy_rep.ocr import extract_text_from_image\n-from .nexy_rep.embed import get_embedding\n-from .nexy_rep.compare import compute_similarity\n-from .nexy_rep.storage import store_data, init_db\n+# Import Nexy-Rep configuration (lazy-import other heavy modules at runtime)\n from .nexy_rep.config import Config\n+# Agentic LLM logic for services\n+from .llm.agent_logic import get_query_generator_chain\n \n class UnifiedService:\n     \"\"\"\n@@ -38,8 +36,18 @@ def __init__(self, config_path: Optional[str] = None):\n                 If not provided, default configuration will be used.\n         \"\"\"\n         # Initialize Nexy-Rep configuration\n-        self.config = Config(config_path) if config_path else Config()\n-        init_db(self.config.db_path)\n+        # Config currently does not accept a path; always instantiate and attach provided path for downstream use.\n+        self.config = Config()\n+        if config_path:\n+            # Attach for consumers that might expect it\n+            setattr(self.config, \"user_config_path\", config_path)\n+        # initialize DB (lazy import to avoid heavy deps during module import)\n+        try:\n+            from .nexy_rep.storage import init_db\n+            init_db(self.config.db_path)\n+        except Exception:\n+            # If storage/init_db can't be imported at module import time, defer until runtime.\n+            logging.debug(\"nexy_rep.storage.init_db not available at import time; will initialize on first store\")\n         \n         # File type to extractor mapping\n         self.extractors = {\n@@ -99,11 +107,27 @@ def capture_and_process_screen(self, store: bool = True) -> Dict[str, Any]:\n         timestamp_str = timestamp.strftime(\"%Y%m%d_%H%M%S\")\n         temp_image_path = os.path.join(self.config.temp_dir, f\"temp_{timestamp_str}.png\")\n         \n-        # Capture screenshot\n-        take_screenshot(temp_image_path)\n-        \n-        # Extract text using OCR\n-        text = extract_text_from_image(temp_image_path)\n+        # Capture screenshot (lazy import to avoid pyautogui at module import)\n+        try:\n+            from .nexy_rep.capture import take_screenshot\n+            take_screenshot(temp_image_path)\n+        except Exception:\n+            # If screenshot capture isn't available, record and return an error-like result\n+            logging.exception(\"Screenshot capture not available\")\n+            return {\"timestamp\": timestamp, \"image_path\": None, \"text\": \"\", \"similarity\": 0.0, \"error\": \"capture_unavailable\"}\n+\n+        # Extract text using OCR (lazy import)\n+        try:\n+            from .nexy_rep.ocr import extract_text_from_image\n+            text = extract_text_from_image(temp_image_path)\n+        except Exception:\n+            logging.exception(\"OCR not available\")\n+            # Clean up temp image if present\n+            try:\n+                os.remove(temp_image_path)\n+            except Exception:\n+                pass\n+            return {\"timestamp\": timestamp, \"image_path\": None, \"text\": \"\", \"similarity\": 0.0, \"error\": \"ocr_unavailable\"}\n         \n         result = {\n             'timestamp': timestamp,\n@@ -116,11 +140,17 @@ def capture_and_process_screen(self, store: bool = True) -> Dict[str, Any]:\n             os.remove(temp_image_path)\n             return result\n         \n-        # Get embedding and compute similarity\n-        embedding = get_embedding(text)\n-        \n-        if self._last_embedding is not None:\n-            result['similarity'] = compute_similarity(embedding, self._last_embedding)\n+        # Get embedding and compute similarity (lazy import)\n+        try:\n+            from .nexy_rep.embed import get_embedding\n+            from .nexy_rep.compare import compute_similarity\n+            embedding = get_embedding(text)\n+            if self._last_embedding is not None:\n+                result['similarity'] = compute_similarity(embedding, self._last_embedding)\n+        except Exception:\n+            logging.exception(\"Embedding/compare not available\")\n+            # If embeddings aren't available, return with zero similarity\n+            embedding = None\n         \n         if store and (self._last_embedding is None or \n                      result['similarity'] < self.config.similarity_threshold):\n@@ -131,12 +161,16 @@ def capture_and_process_screen(self, store: bool = True) -> Dict[str, Any]:\n             )\n             os.rename(temp_image_path, permanent_image_path)\n             \n-            store_data(\n-                self.config.db_path,\n-                timestamp=timestamp,\n-                image_path=permanent_image_path,\n-                extracted_text=text\n-            )\n+            try:\n+                from .nexy_rep.storage import store_data\n+                store_data(\n+                    self.config.db_path,\n+                    timestamp=timestamp,\n+                    image_path=permanent_image_path,\n+                    extracted_text=text\n+                )\n+            except Exception:\n+                logging.exception(\"Failed to store data to nexy_rep.storage\")\n             \n             result['image_path'] = permanent_image_path\n             self._last_embedding = embedding\n@@ -164,8 +198,13 @@ def process_image(self, image_path: str, store: bool = True) -> Dict[str, Any]:\n         \"\"\"\n         timestamp = datetime.now()\n         \n-        # Extract text using OCR\n-        text = extract_text_from_image(image_path)\n+        # Extract text using OCR (lazy import)\n+        try:\n+            from .nexy_rep.ocr import extract_text_from_image\n+            text = extract_text_from_image(image_path)\n+        except Exception:\n+            logging.exception(\"OCR not available for process_image\")\n+            return {\"timestamp\": timestamp, \"image_path\": None, \"text\": \"\", \"similarity\": 0.0, \"error\": \"ocr_unavailable\"}\n         \n         result = {\n             'timestamp': timestamp,\n@@ -177,11 +216,16 @@ def process_image(self, image_path: str, store: bool = True) -> Dict[str, Any]:\n         if not text.strip():\n             return result\n         \n-        # Get embedding and compute similarity\n-        embedding = get_embedding(text)\n-        \n-        if self._last_embedding is not None:\n-            result['similarity'] = compute_similarity(embedding, self._last_embedding)\n+        # Get embedding and compute similarity (lazy import)\n+        try:\n+            from .nexy_rep.embed import get_embedding\n+            from .nexy_rep.compare import compute_similarity\n+            embedding = get_embedding(text)\n+            if self._last_embedding is not None:\n+                result['similarity'] = compute_similarity(embedding, self._last_embedding)\n+        except Exception:\n+            logging.exception(\"Embedding/compare not available for process_image\")\n+            embedding = None\n         \n         if store and (self._last_embedding is None or \n                      result['similarity'] < self.config.similarity_threshold):\n@@ -196,14 +240,57 @@ def process_image(self, image_path: str, store: bool = True) -> Dict[str, Any]:\n             import shutil\n             shutil.copy2(image_path, permanent_image_path)\n             \n-            store_data(\n-                self.config.db_path,\n-                timestamp=timestamp,\n-                image_path=permanent_image_path,\n-                extracted_text=text\n-            )\n+            try:\n+                from .nexy_rep.storage import store_data\n+                store_data(\n+                    self.config.db_path,\n+                    timestamp=timestamp,\n+                    image_path=permanent_image_path,\n+                    extracted_text=text\n+                )\n+            except Exception:\n+                logging.exception(\"Failed to store data to nexy_rep.storage in process_image\")\n             \n             result['image_path'] = permanent_image_path\n             self._last_embedding = embedding\n         \n         return result\n+\n+    def run_agentic_query(\n+        self,\n+        context: str,\n+        question: str,\n+        model_name: Optional[str] = None,\n+        base_url: Optional[str] = None,\n+        api_key: Optional[str] = None,\n+    ) -> Any:\n+        \"\"\"\n+        Run the agentic LLM chain on provided context and question.\n+\n+        This wraps the existing `get_query_generator_chain` from\n+        `services/llm/agent_logic.py` and invokes the chain with the\n+        given inputs. Returns whatever the chain returns (typically a\n+        parsed Pydantic object or a dict-like result).\n+\n+        Args:\n+            context: The textual context to provide to the agent.\n+            question: The human question or instruction for the agent.\n+            model_name: Optional model identifier (falls back to Ollama/local behavior if not provided).\n+            base_url: Optional base URL for remote LLM endpoints (used by some adapters).\n+            api_key: Optional API key for hosted models (e.g., Gemini).\n+\n+        Returns:\n+            Any: The chain invocation result. On error, returns a dict with an 'error' key.\n+        \"\"\"\n+        # Choose a reasonable default model if none provided. We prefer not to force a hosted model here.\n+        model_to_use = model_name or os.environ.get(\"NEXA_DEFAULT_MODEL\")\n+\n+        try:\n+            chain = get_query_generator_chain(model_name=model_to_use or \"ollama\", base_url=base_url, api_key=api_key)\n+            # The chain API in this project uses .invoke with a dict carrying context and question\n+            res = chain.invoke({\"context\": context, \"question\": question})\n+            return res\n+        except Exception as exc:  # pragma: no cover - surface runtime errors\n+            # Keep failure mode explicit for callers\n+            logging.exception(\"Agentic query failed\")\n+            return {\"error\": str(exc)}"
            },
            {
                "sha": "4d93d3136973f7ede6285d34c02d8afa99bd5f19",
                "filename": "test.ipynb",
                "status": "modified",
                "additions": 22,
                "deletions": 39,
                "changes": 61,
                "blob_url": "https://github.com/Rajasimhareddybolla/Nexa/blob/7039511012d29f778f46f78c280ebcd975390ccd/test.ipynb",
                "raw_url": "https://github.com/Rajasimhareddybolla/Nexa/raw/7039511012d29f778f46f78c280ebcd975390ccd/test.ipynb",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/Nexa/contents/test.ipynb?ref=7039511012d29f778f46f78c280ebcd975390ccd",
                "patch": "@@ -2,7 +2,7 @@\n  \"cells\": [\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 16,\n+   \"execution_count\": 1,\n    \"id\": \"d6b3bb1b\",\n    \"metadata\": {},\n    \"outputs\": [],\n@@ -101,76 +101,59 @@\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 1,\n+   \"execution_count\": 2,\n    \"id\": \"85d9c333\",\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"from llm.agent_logic import get_query_generator_chain\"\n-   ]\n-  },\n-  {\n-   \"cell_type\": \"code\",\n-   \"execution_count\": 14,\n-   \"id\": \"cd4c19e3\",\n-   \"metadata\": {},\n-   \"outputs\": [],\n-   \"source\": [\n-    \"chain = get_query_generator_chain(model_name=\\\"gemini-2.5-flash\\\", api_key=\\\"AIzaSyCN7_xw5_1AXxYU90oXfrsWC7HxIam-hMM\\\")\"\n+    \"from services.services import UnifiedService\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 17,\n-   \"id\": \"92165680\",\n+   \"execution_count\": 3,\n+   \"id\": \"a1fd2cc6\",\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"res  = chain.invoke(\\n\",\n-    \"    {\\n\",\n-    \"        \\\"context\\\": emp_,\\n\",\n-    \"        \\\"question\\\": \\\"give me the task list to be done by jdoe\\\"\\n\",\n-    \"    }\\n\",\n-    \")\"\n+    \"ser = UnifiedService()\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 11,\n-   \"id\": \"2ade4cfb\",\n+   \"execution_count\": 4,\n+   \"id\": \"cd4c19e3\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n-     \"data\": {\n-      \"text/plain\": [\n-       \"OutputFormat(todays_focus='The primary goals are to finalize the project proposal, review budget allocations, address sprint issues, align team responsibilities, and schedule team-building. Additionally, the AWS account issue must be resolved and the budget report prepared today.', tasks=[])\"\n-      ]\n-     },\n-     \"execution_count\": 11,\n-     \"metadata\": {},\n-     \"output_type\": \"execute_result\"\n+     \"name\": \"stderr\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"/Users/raja/Desktop/Nexa/env/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.0) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\\n\",\n+      \"  warnings.warn(message, FutureWarning)\\n\"\n+     ]\n     }\n    ],\n    \"source\": [\n-    \"res\"\n+    \"res = ser.run_agentic_query(context=emp_ , question=\\\"create task and prioritise tasks for today along, with the time to complete in b/w 9 to 5\\\" , api_key=\\\"AIzaSyCN7_xw5_1AXxYU90oXfrsWC7HxIam-hMM\\\" , model_name=\\\"gemini-2.5-flash\\\")\"\n    ]\n   },\n   {\n    \"cell_type\": \"code\",\n-   \"execution_count\": 18,\n-   \"id\": \"a2dcebe7\",\n+   \"execution_count\": 7,\n+   \"id\": \"92165680\",\n    \"metadata\": {},\n    \"outputs\": [\n     {\n      \"data\": {\n       \"text/plain\": [\n-       \"[Task(task='Finish Redis caching integration for the search API', priority='High', etr='4 hours', status_comments='Status: In Progress. Explicitly stated as top priority in stand-up. Active development detected (per OCR at 10:15Z on api-cache.js).'),\\n\",\n-       \" Task(task='Apply CVE-2025-1234 security patch for auth endpoints', priority='High', etr='Not Stated', status_comments='Status: Not Started. New URGENT requirement from security-audit-q4.pdf and confirmed as high priority in stand-up, planned after caching.'),\\n\",\n-       \" Task(task='Implement OAuth refresh token handling (Issue #456)', priority='Medium', etr='Not Stated', status_comments='Status: Not Started. Assigned as issue #456, due EOD today and planned after the security patch.'),\\n\",\n-       \" Task(task='Resolve database migration script schema conflicts for users schema', priority='Medium', etr='Blocked - Time Pending', status_comments=\\\"Status: Blocked. Carry-over from yesterday due to schema conflicts. Latest OCR (14:30Z) confirms the migration attempt failed due to persistent schema conflicts, despite @bsmith's earlier commitment to review the script.\\\")]\"\n+       \"[Task(task='Finish caching integration with Redis', priority='High', etr='4 hours', status_comments='Status: In Progress (per OCR). Explicitly stated as top priority.'),\\n\",\n+       \" Task(task='Apply CVE-2025-1234 patch for auth endpoints', priority='High', etr='Not Stated or Blocked - Time Pending', status_comments='Status: Planned. New URGENT requirement from security audit document and high priority per stand-up, to be actioned post-caching.'),\\n\",\n+       \" Task(task='Implement OAuth refresh token handling (Issue #456)', priority='Medium', etr='Not Stated or Blocked - Time Pending', status_comments='Status: Planned. Assigned via GitHub and due EOD, to be actioned post-security patch.'),\\n\",\n+       \" Task(task='Resolve database migration schema conflicts and apply script', priority='Low', etr='Not Stated or Blocked - Time Pending', status_comments='Status: Blocked by schema conflict (per OCR). Requires DBA input for resolution, carried over from yesterday.')]\"\n       ]\n      },\n-     \"execution_count\": 18,\n+     \"execution_count\": 7,\n      \"metadata\": {},\n      \"output_type\": \"execute_result\"\n     }"
            }
        ],
        "parents": [
            "60ade733e9c40f37bd51970fdf089880dc3b6525"
        ]
    },
    {
        "repository": "Rajasimhareddybolla/nexa",
        "sha": "09eff1c47ff13eabcdb33bdecf01a80bbc2d874b",
        "message": "added llm one",
        "author": {
            "name": "raja",
            "email": "rajasimhabolla@gmail.com",
            "date": "2025-11-08T05:16:30Z"
        },
        "committer": {
            "name": "raja",
            "email": "rajasimhabolla@gmail.com",
            "date": "2025-11-08T05:16:30Z"
        },
        "stats": {
            "total": 433,
            "additions": 433,
            "deletions": 0
        },
        "files": [
            {
                "sha": "ae412d6a077a62952050d429f089e4da4aee017b",
                "filename": ".gitignore",
                "status": "added",
                "additions": 1,
                "deletions": 0,
                "changes": 1,
                "blob_url": "https://github.com/Rajasimhareddybolla/Nexa/blob/09eff1c47ff13eabcdb33bdecf01a80bbc2d874b/.gitignore",
                "raw_url": "https://github.com/Rajasimhareddybolla/Nexa/raw/09eff1c47ff13eabcdb33bdecf01a80bbc2d874b/.gitignore",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/Nexa/contents/.gitignore?ref=09eff1c47ff13eabcdb33bdecf01a80bbc2d874b",
                "patch": "@@ -0,0 +1 @@\n+env/\n\\ No newline at end of file"
            },
            {
                "sha": "6903b1de6279c789f8aa945193fb2dd65b66da6e",
                "filename": "llm/__pycache__/agent_logic.cpython-310.pyc",
                "status": "added",
                "additions": 0,
                "deletions": 0,
                "changes": 0,
                "blob_url": "https://github.com/Rajasimhareddybolla/Nexa/blob/09eff1c47ff13eabcdb33bdecf01a80bbc2d874b/llm%2F__pycache__%2Fagent_logic.cpython-310.pyc",
                "raw_url": "https://github.com/Rajasimhareddybolla/Nexa/raw/09eff1c47ff13eabcdb33bdecf01a80bbc2d874b/llm%2F__pycache__%2Fagent_logic.cpython-310.pyc",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/Nexa/contents/llm%2F__pycache__%2Fagent_logic.cpython-310.pyc?ref=09eff1c47ff13eabcdb33bdecf01a80bbc2d874b"
            },
            {
                "sha": "008e6fcffc82067a4c5a9af45a9106f35af7f412",
                "filename": "llm/agent_logic.py",
                "status": "added",
                "additions": 72,
                "deletions": 0,
                "changes": 72,
                "blob_url": "https://github.com/Rajasimhareddybolla/Nexa/blob/09eff1c47ff13eabcdb33bdecf01a80bbc2d874b/llm%2Fagent_logic.py",
                "raw_url": "https://github.com/Rajasimhareddybolla/Nexa/raw/09eff1c47ff13eabcdb33bdecf01a80bbc2d874b/llm%2Fagent_logic.py",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/Nexa/contents/llm%2Fagent_logic.py?ref=09eff1c47ff13eabcdb33bdecf01a80bbc2d874b",
                "patch": "@@ -0,0 +1,72 @@\n+import os\n+import sqlite3\n+import contextlib\n+import io\n+import logging\n+from typing import List, Optional, Dict, Any\n+\n+from langchain_core.prompts import ChatPromptTemplate\n+from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n+from pydantic import BaseModel, Field\n+\n+\n+# module logger\n+logger = logging.getLogger(__name__)\n+if not logger.handlers:\n+    logging.basicConfig(level=logging.INFO)\n+\n+class Task(BaseModel):\n+    \"\"\"A model to represent a single task in the output format.\"\"\"\n+    task: str = Field(description=\"The description of the specific task.\")\n+    priority: str = Field(description=\"The priority level: High, Medium, or Low.\")\n+    etr: str = Field(description=\"The stated estimated time remaining, e.g., '3 hours' or 'Not Stated'.\")\n+    status_comments: str = Field(description=\"The status and comments, e.g., 'Status: In Progress (per OCR)'.\")\n+\n+class OutputFormat(BaseModel):\n+    \"\"\"The structured output format for the response.\"\"\"\n+    todays_focus: str = Field(description=\"1-2 sentence summary of the day\u2019s primary goal, the most critical item, and the highest-priority blocker.\")\n+    tasks: List[Task] = Field(description=\"A list of remaining tasks with their details.\")\n+    \n+def get_prompt(file):\n+    with open(file, 'r') as f:\n+        prompt = f.read()\n+    # Escape curly braces to prevent parsing errors in f-string format\n+    prompt = prompt.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n+    return prompt\n+\n+def get_llm(model_name: str, base_url: Optional[str] = None, api_key: Optional[str] = None):\n+    \"\"\"Dynamically loads and returns the appropriate LLM based on the model name.\"\"\"\n+    if model_name.startswith(\"gemini\"):\n+        from langchain_google_genai import ChatGoogleGenerativeAI\n+        if api_key is None:\n+            raise ValueError(\"API key is required for Gemini models.\")\n+        return ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key)\n+    else:\n+        # Assume local model via Ollama\n+        from langchain_ollama import OllamaLLM as Ollama\n+        if base_url is None:\n+            base_url = \"http://host.docker.internal:11434\"  # Default base URL\n+        return Ollama(model=model_name, base_url=base_url)\n+\n+def get_query_generator_chain(model_name: str, base_url: Optional[str] = None, api_key: Optional[str] = None):\n+    \"\"\"Builds the chain for Agent 1 (Query Generation).\"\"\"\n+    \n+    llm = get_llm(model_name, base_url=base_url, api_key=api_key)\n+\n+    parser = PydanticOutputParser(pydantic_object=OutputFormat)\n+\n+    system_prompt_1 = get_prompt(\"llm/assets/system_instructions.md\")\n+\n+    format_instructions = parser.get_format_instructions()\n+    # Escape curly braces in format_instructions\n+    format_instructions = format_instructions.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n+\n+    # Append format instructions to the system prompt\n+    system_prompt_1 += f\"\\n\\n{format_instructions}\"\n+\n+    prompt_template = ChatPromptTemplate.from_messages([\n+        (\"system\", system_prompt_1),\n+        (\"human\", \"Data Context:\\n{context}\\n\\nUser Question:\\n{question}\")\n+    ])\n+    \n+    return prompt_template | llm | parser\n\\ No newline at end of file"
            },
            {
                "sha": "b5bd094d16d43cc4ea4b03389bab3915cc7e35bd",
                "filename": "llm/assets/system_instructions.md",
                "status": "added",
                "additions": 143,
                "deletions": 0,
                "changes": 143,
                "blob_url": "https://github.com/Rajasimhareddybolla/Nexa/blob/09eff1c47ff13eabcdb33bdecf01a80bbc2d874b/llm%2Fassets%2Fsystem_instructions.md",
                "raw_url": "https://github.com/Rajasimhareddybolla/Nexa/raw/09eff1c47ff13eabcdb33bdecf01a80bbc2d874b/llm%2Fassets%2Fsystem_instructions.md",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/Nexa/contents/llm%2Fassets%2Fsystem_instructions.md?ref=09eff1c47ff13eabcdb33bdecf01a80bbc2d874b",
                "patch": "@@ -0,0 +1,143 @@\n+Persona\n+\n+You are a \"Project-Chronos\" AI, a highly specialized, analytical project management engine. You function as the Chief of Staff AI, providing objective, data-driven task analysis. Your tone is concise, directive, and strictly non-judgmental. Your sole function is to analyze the provided data for the target employee and produce the plan; you will not editorialize, use casual language, or offer encouragement.\n+\n+0. Target Employee and Filtering\n+\n+You MUST filter all data (GitHub, transcripts, OCR) to focus only on the provided employee ID. This is the target_employee_username (e.g., \"jdoe\"). Ignore actions, issues, or dialogue initiated by other users unless they explicitly block the target employee on an active task.\n+\n+1. Objective\n+\n+Analyze the provided structured input data for the target_employee_username to produce a clear, prioritized daily task plan. The final output MUST be a 1-2 sentence summary followed immediately by the specified Markdown table, with absolutely no other text.\n+\n+2. Input Data Structure (MANDATORY Format)\n+\n+The input you receive is guaranteed to be a structured array of event objects, ensuring the context of every data point is known. You must parse and use these type and timestamp fields for reliable analysis.\n+\n+Required Input Format Example:\n+\n+[\n+  {\n+    \"type\": \"meeting_transcript\",\n+    \"timestamp\": \"2024-10-28T16:30:00Z\",\n+    \"data\": { \"purpose\": \"Evening Review\", \"content\": \"jdoe: Today I completed the API endpoint, but I didn\u2019t get to the documentation.\" }\n+  },\n+  {\n+    \"type\": \"github_action\",\n+    \"timestamp\": \"2024-10-29T09:15:00Z\",\n+    \"data\": { \"user\": \"jdoe\", \"action\": \"commit\", \"branch\": \"feat/login-v2\", \"message\": \"fix: resolve auth-loop\" }\n+  },\n+  {\n+    \"type\": \"meeting_transcript\",\n+    \"timestamp\": \"2024-10-29T09:45:00Z\",\n+    \"data\": { \"purpose\": \"Morning Stand-up\", \"content\": \"jdoe: My top priority is to finish login-v2, which should take 3 hours. I am blocked by @asmith's review.\" }\n+  },\n+  {\n+    \"type\": \"ocr_capture\",\n+    \"timestamp\": \"2024-10-29T10:30:00Z\",\n+    \"data\": { \"window_title\": \"VS Code - [project-zeta/auth.js]\", \"content\": \"function handleAuthRequest(req, res) { ... // active function code\" }\n+  },\n+  {\n+    \"type\": \"doc_scrape\",\n+    \"timestamp\": \"2024-10-29T11:00:00Z\",\n+    \"data\": { \"source\": \"api-specs-v3.pdf\", \"content\": \"NEW URGENT REQUIREMENT: All endpoints must now support OAuth 2.0.\" }\n+  }\n+]\n+\n+\n+3. Core Processing Logic & Rules\n+\n+A. Data Filtering & Noise Reduction\n+\n+Employee Filter: Only process tasks/actions where the primary actor is the target_employee_username.\n+\n+OCR Noise Filter: Ignore ocr_capture events where the window_title contains: [\"Slack\", \"Email\", \"Outlook\", \"Google Calendar\", \"Spotify\", \"Teams\", \"Zoom\", \"Social Media\"]. Only productive application windows (IDE, terminal, documentation viewer) are relevant.\n+\n+B. Establish State and Continuity\n+\n+Carry-over Tasks: Use the latest Evening Review transcript to extract any task explicitly stated as \"incomplete,\" \"didn't finish,\" or \"carry to tomorrow.\" These become tasks for today.\n+\n+Completed Tasks: Tasks mentioned as \"completed\" in the Evening Review or verified by a merged PR action are marked DONE and must not appear in the output task table.\n+\n+C. Identify Today's Planned Tasks (Source of Truth)\n+\n+The latest Morning Stand-up is the highest authority for the day's intent. Extract all explicit tasks and blockers.\n+\n+Any task not mentioned in the stand-up but appearing as a new assigned issue # or a new URGENT doc requirement is a New Task.\n+\n+D. Conflict Resolution and Verification\n+\n+Hierarchy of Truth: Stand-up (Intent) > GitHub Action (Verification) > OCR Capture (Active Focus).\n+\n+Conflict Handling: If the Morning Stand-up states Task A is the plan, but OCR shows the employee active on Task B (unrelated or Low Priority), the comment for Task A must explicitly state: Status: Not Started. Active deviation detected (working on Task B per OCR).\n+\n+Completion Conflict: If the stand-up states a task is done, but no corresponding GitHub activity (commit, PR) exists, the comment must state: Status: Conflicting. Reported Done, but no technical evidence found.\n+\n+E. Priority Rules\n+| Priority | Rule |\n+| :--- | :--- |\n+| High | Task is a blocker for another team member, involves a critical bug, is a new urgent requirement from a document scrape, or was explicitly labeled \"top priority\" in the stand-up. |\n+| Medium | Planned feature development, planned ongoing tasks without external urgency, or non-critical carry-over tasks. |\n+| Low | Documentation updates, refactoring, research, or non-urgent technical debt items. |\n+\n+F. Time Estimation (CRITICAL FIX: Reporting Only)\n+\n+The column Stated ETR (Estimated Time Remaining) MUST NOT be guessed.\n+\n+If the employee explicitly provided a time estimate in the Morning Stand-up (e.g., \"should take 3 hours\"), use that exact text.\n+\n+In all other cases, or if the task is blocked, the field MUST be: Not Stated or Blocked - Time Pending.\n+\n+4. Fail-Safe Behavior (Strict Fallbacks)\n+\n+Empty Input: If the input array is empty, your entire output must be: No data received for [target_employee_username].\n+\n+Missing Morning Stand-up: If this core document is missing, the Today's Focus summary must begin with: ATTENTION: No Morning Stand-up transcript was found. Plan inferred from yesterday's review and live activity.\n+\n+Uncertainty: If a task's status or priority cannot be confidently determined, state the uncertainty explicitly in the Status & Comments column.\n+\n+5. Output Format (Mandatory & Strict)\n+\n+Your entire response MUST adhere to this format.\n+\n+Today's Focus: [1-2 sentence summary of the day\u2019s primary goal, the most critical item, and the highest-priority blocker.]\n+\n+Tasks Remaining\n+\n+Priority\n+\n+Stated ETR\n+\n+Status & Comments\n+\n+[Specific Task 1]\n+\n+[High/Medium/Low]\n+\n+[e.g., \"3 hours\" / \"Not Stated\"]\n+\n+[e.g., \"Status: In Progress (per OCR)\" / \"Status: Blocked by @asmith\" / \"Status: New URGENT requirement from PDF.\"]\n+\n+[Specific Task 2]\n+\n+[High/Medium/Low]\n+\n+[e.g., \"Blocked - Time Pending\"]\n+\n+[e.g., \"Status: Carry-over. Low Priority as per stand-up.\"]\n+\n+## always follow this response schemas\n+\n+\n+class Task(BaseModel):\n+    \"\"\"A model to represent a single task in the output format.\"\"\"\n+    task: str = Field(description=\"The description of the specific task.\")\n+    priority: str = Field(description=\"The priority level: High, Medium, or Low.\")\n+    etr: str = Field(description=\"The stated estimated time remaining, e.g., '3 hours' or 'Not Stated'.\")\n+    status_comments: str = Field(description=\"The status and comments, e.g., 'Status: In Progress (per OCR)'.\")\n+\n+class OutputFormat(BaseModel):\n+    \"\"\"The structured output format for the response.\"\"\"\n+    todays_focus: str = Field(description=\"1-2 sentence summary of the day\u2019s primary goal, the most critical item, and the highest-priority blocker.\")\n+    tasks: List[Task] = Field(description=\"A list of remaining tasks with their details.\")\n+    "
            },
            {
                "sha": "913454ddf570e1a8e00fff4e7d1ce16cd2ceb003",
                "filename": "requirements.txt",
                "status": "added",
                "additions": 5,
                "deletions": 0,
                "changes": 5,
                "blob_url": "https://github.com/Rajasimhareddybolla/Nexa/blob/09eff1c47ff13eabcdb33bdecf01a80bbc2d874b/requirements.txt",
                "raw_url": "https://github.com/Rajasimhareddybolla/Nexa/raw/09eff1c47ff13eabcdb33bdecf01a80bbc2d874b/requirements.txt",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/Nexa/contents/requirements.txt?ref=09eff1c47ff13eabcdb33bdecf01a80bbc2d874b",
                "patch": "@@ -0,0 +1,5 @@\n+langchain\n+langchain_core\n+pydantic\n+langchain-ollama\n+langchain-google-genai\n\\ No newline at end of file"
            },
            {
                "sha": "83ceb03de39b8de458867f4828f0980a6a4554f3",
                "filename": "test.ipynb",
                "status": "added",
                "additions": 212,
                "deletions": 0,
                "changes": 212,
                "blob_url": "https://github.com/Rajasimhareddybolla/Nexa/blob/09eff1c47ff13eabcdb33bdecf01a80bbc2d874b/test.ipynb",
                "raw_url": "https://github.com/Rajasimhareddybolla/Nexa/raw/09eff1c47ff13eabcdb33bdecf01a80bbc2d874b/test.ipynb",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/Nexa/contents/test.ipynb?ref=09eff1c47ff13eabcdb33bdecf01a80bbc2d874b",
                "patch": "@@ -0,0 +1,212 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 16,\n+   \"id\": \"d6b3bb1b\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"emp_ = \\\"\\\"\\\"```json\\n\",\n+    \"[\\n\",\n+    \"  {\\n\",\n+    \"    \\\"type\\\": \\\"meeting_transcript\\\",\\n\",\n+    \"    \\\"timestamp\\\": \\\"2025-11-07T18:00:00Z\\\",\\n\",\n+    \"    \\\"data\\\": {\\n\",\n+    \"      \\\"purpose\\\": \\\"Evening Review\\\",\\n\",\n+    \"      \\\"content\\\": \\\"Team Lead: Alright, team, let's wrap up for the day. Starting with jdoe - what did you accomplish today, any blockers, and what's carrying over?\\\\njdoe: Today, I focused on the backend scaling for the search API. I completed the initial load testing using JMeter, simulating 10k concurrent users, and identified bottlenecks in the query optimization. I pushed commits to the feat/api-scaling branch with optimizations like index additions and query caching hints. However, I didn't finish integrating the new Redis caching layer because I ran into configuration issues with the cluster setup in our GCP environment. Also, the database migration script for the users schema is still pending - I have the script ready, but there are schema conflicts with the existing prod data that need resolution. I spent about 2 hours debugging that but couldn't resolve it without input from the DBA team. So, those two are carrying over to tomorrow.\\\\nTeam Lead: Noted. @bsmith, can you review jdoe's migration script first thing tomorrow?\\\\n@bsmith: Yes, I'll prioritize that.\\\"\\n\",\n+    \"    }\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    \\\"type\\\": \\\"github_action\\\",\\n\",\n+    \"    \\\"timestamp\\\": \\\"2025-11-07T20:45:00Z\\\",\\n\",\n+    \"    \\\"data\\\": {\\n\",\n+    \"      \\\"user\\\": \\\"jdoe\\\",\\n\",\n+    \"      \\\"action\\\": \\\"commit\\\",\\n\",\n+    \"      \\\"branch\\\": \\\"feat/api-scaling\\\",\\n\",\n+    \"      \\\"message\\\": \\\"add: preliminary scaling optimizations\\\\n\\\\n- Added composite indexes on frequently queried fields in the search table.\\\\n- Implemented connection pooling for database queries to reduce overhead.\\\\n- Updated config for higher thread counts in the API server.\\\\n\\\\nTested locally with 5k simulated requests; response time improved by 40%.\\\"\\n\",\n+    \"    }\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    \\\"type\\\": \\\"github_action\\\",\\n\",\n+    \"    \\\"timestamp\\\": \\\"2025-11-07T21:15:00Z\\\",\\n\",\n+    \"    \\\"data\\\": {\\n\",\n+    \"      \\\"user\\\": \\\"jdoe\\\",\\n\",\n+    \"      \\\"action\\\": \\\"pr_opened\\\",\\n\",\n+    \"      \\\"pr_id\\\": 789,\\n\",\n+    \"      \\\"title\\\": \\\"Feat: API Scaling Optimizations\\\",\\n\",\n+    \"      \\\"description\\\": \\\"This PR introduces initial scaling improvements for the backend search API. Includes indexing, pooling, and config tweaks. Awaiting review before merge. Related to issue #123.\\\"\\n\",\n+    \"    }\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    \\\"type\\\": \\\"meeting_transcript\\\",\\n\",\n+    \"    \\\"timestamp\\\": \\\"2025-11-08T09:30:00Z\\\",\\n\",\n+    \"    \\\"data\\\": {\\n\",\n+    \"      \\\"purpose\\\": \\\"Morning Stand-up\\\",\\n\",\n+    \"      \\\"content\\\": \\\"Team Lead: Good morning, team. Quick stand-up: Yesterday, today, blockers. jdoe, go ahead.\\\\njdoe: Yesterday, as mentioned in the evening review, I completed the scaling tests and optimizations but left the Redis integration and DB migration unfinished. Today, my top priority is finishing the caching integration with Redis - I estimate about 4 hours for that, including testing in staging. After that, I'll tackle the database migration, which is blocked pending @bsmith's review on the schema conflicts. Also, I saw the new security audit doc; the CVE-2025-1234 patch for our auth endpoints is urgent, so I'll slot that in as high priority once caching is done. Additionally, I got assigned issue #456 for OAuth refresh token handling - that's due EOD, so medium priority after the patch.\\\\nTeam Lead: Sounds good. @bsmith, get that review done ASAP.\\\\n@bsmith: On it, should be done by 10 AM.\\\"\\n\",\n+    \"    }\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    \\\"type\\\": \\\"ocr_capture\\\",\\n\",\n+    \"    \\\"timestamp\\\": \\\"2025-11-08T10:15:00Z\\\",\\n\",\n+    \"    \\\"data\\\": {\\n\",\n+    \"      \\\"window_title\\\": \\\"VS Code - [google-backend/api-cache.js]\\\",\\n\",\n+    \"      \\\"content\\\": \\\"// api-cache.js\\\\nimport Redis from 'ioredis';\\\\nconst redisClient = new Redis({ host: 'redis-cluster.google.internal', port: 6379 });\\\\n\\\\nfunction setupCacheLayer(client) {\\\\n  // Set up TTL for cache entries\\\\n  const TTL = 3600; // 1 hour\\\\n  client.on('error', (err) => console.error('Redis Client Error', err));\\\\n\\\\n  async function cacheQuery(key, queryFn) {\\\\n    const cached = await redisClient.get(key);\\\\n    if (cached) return JSON.parse(cached);\\\\n    const result = await queryFn();\\\\n    await redisClient.set(key, JSON.stringify(result), 'EX', TTL);\\\\n    return result;\\\\n  }\\\\n  // TODO: Handle cache invalidation on data updates\\\\n  return { cacheQuery };\\\\n}\\\\n\\\\nexport default setupCacheLayer;\\\"\\n\",\n+    \"    }\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    \\\"type\\\": \\\"github_action\\\",\\n\",\n+    \"    \\\"timestamp\\\": \\\"2025-11-08T11:00:00Z\\\",\\n\",\n+    \"    \\\"data\\\": {\\n\",\n+    \"      \\\"user\\\": \\\"team-lead\\\",\\n\",\n+    \"      \\\"action\\\": \\\"issue_assigned\\\",\\n\",\n+    \"      \\\"issue_id\\\": 456,\\n\",\n+    \"      \\\"title\\\": \\\"Implement OAuth refresh token handling\\\",\\n\",\n+    \"      \\\"description\\\": \\\"Backend needs to support automatic refresh of OAuth tokens for long-lived sessions. Include error handling for revoked tokens. Assigned to jdoe; priority medium, due EOD today. Steps:\\\\n1. Update auth middleware to check token expiry.\\\\n2. Integrate refresh endpoint with Google OAuth API.\\\\n3. Add unit tests for refresh flow.\\\\n4. Deploy to staging for verification.\\\"\\n\",\n+    \"    }\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    \\\"type\\\": \\\"doc_scrape\\\",\\n\",\n+    \"    \\\"timestamp\\\": \\\"2025-11-08T12:00:00Z\\\",\\n\",\n+    \"    \\\"data\\\": {\\n\",\n+    \"      \\\"source\\\": \\\"security-audit-q4.pdf\\\",\\n\",\n+    \"      \\\"content\\\": \\\"Security Audit Report - Q4 2025\\\\n\\\\nExecutive Summary: Critical vulnerabilities identified in backend services.\\\\n\\\\nURGENT ACTION REQUIRED:\\\\n- CVE-2025-1234: Vulnerability in auth library allowing token replay attacks. Patch by updating to version 5.2.1 and enforcing nonce checks.\\\\n- Apply to all endpoints immediately; test in prod-like environment.\\\\n- Responsible: Backend Team (jdoe lead on auth).\\\\n\\\\nAdditional Recommendations:\\\\n- Enable full logging for auth events.\\\\n- Schedule penetration test post-patch.\\\"\\n\",\n+    \"    }\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    \\\"type\\\": \\\"ocr_capture\\\",\\n\",\n+    \"    \\\"timestamp\\\": \\\"2025-11-08T14:30:00Z\\\",\\n\",\n+    \"    \\\"data\\\": {\\n\",\n+    \"      \\\"window_title\\\": \\\"Terminal - gcloud db migrate\\\",\\n\",\n+    \"      \\\"content\\\": \\\"$ gcloud sql instances describe backend-db\\\\n...\\\\n$ ./migrate.sh --schema users\\\\nRunning migration...\\\\nApplying change: ALTER TABLE users ADD COLUMN oauth_refresh_token VARCHAR(255);\\\\nError: Schema conflict on users table - duplicate column detected from partial migration last week.\\\\nSuggestion: Run with --force to override, but risk data loss.\\\\nAborting migration.\\\"\\n\",\n+    \"    }\\n\",\n+    \"  },\\n\",\n+    \"  {\\n\",\n+    \"    \\\"type\\\": \\\"github_action\\\",\\n\",\n+    \"    \\\"timestamp\\\": \\\"2025-11-08T15:00:00Z\\\",\\n\",\n+    \"    \\\"data\\\": {\\n\",\n+    \"      \\\"user\\\": \\\"bsmith\\\",\\n\",\n+    \"      \\\"action\\\": \\\"pr_review\\\",\\n\",\n+    \"      \\\"pr_id\\\": 789,\\n\",\n+    \"      \\\"comment\\\": \\\"Reviewed: Looks good, but add more comments on the pooling config. Approved with changes.\\\"\\n\",\n+    \"    }\\n\",\n+    \"  }\\n\",\n+    \"]\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"**Target Employee Username:** jdoe (Google Backend Engineer handling intense workload in API scaling, security patching, and auth integrations, with personalization from yesterday's carry-over tasks like incomplete Redis caching and DB migration conflicts).\\n\",\n+    \"\\n\",\n+    \"**User Question to Ask the AI:** Analyze the detailed data for jdoe from yesterday and today, including full meeting transcripts, GitHub actions with commit messages and descriptions, OCR captures of code and terminal sessions, and document scrapes. Generate a prioritized daily task plan emphasizing backend optimizations, urgent security patches, OAuth handling, and resolving migration blockers under high-pressure deadlines, ensuring carry-over tasks from yesterday are addressed with detailed status updates resembling a real engineer's workflow.\\\"\\\"\\\"\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 1,\n+   \"id\": \"85d9c333\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from llm.agent_logic import get_query_generator_chain\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 14,\n+   \"id\": \"cd4c19e3\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"chain = get_query_generator_chain(model_name=\\\"gemini-2.5-flash\\\", api_key=\\\"AIzaSyCN7_xw5_1AXxYU90oXfrsWC7HxIam-hMM\\\")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 17,\n+   \"id\": \"92165680\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"res  = chain.invoke(\\n\",\n+    \"    {\\n\",\n+    \"        \\\"context\\\": emp_,\\n\",\n+    \"        \\\"question\\\": \\\"give me the task list to be done by jdoe\\\"\\n\",\n+    \"    }\\n\",\n+    \")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 11,\n+   \"id\": \"2ade4cfb\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"data\": {\n+      \"text/plain\": [\n+       \"OutputFormat(todays_focus='The primary goals are to finalize the project proposal, review budget allocations, address sprint issues, align team responsibilities, and schedule team-building. Additionally, the AWS account issue must be resolved and the budget report prepared today.', tasks=[])\"\n+      ]\n+     },\n+     \"execution_count\": 11,\n+     \"metadata\": {},\n+     \"output_type\": \"execute_result\"\n+    }\n+   ],\n+   \"source\": [\n+    \"res\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": 18,\n+   \"id\": \"a2dcebe7\",\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"data\": {\n+      \"text/plain\": [\n+       \"[Task(task='Finish Redis caching integration for the search API', priority='High', etr='4 hours', status_comments='Status: In Progress. Explicitly stated as top priority in stand-up. Active development detected (per OCR at 10:15Z on api-cache.js).'),\\n\",\n+       \" Task(task='Apply CVE-2025-1234 security patch for auth endpoints', priority='High', etr='Not Stated', status_comments='Status: Not Started. New URGENT requirement from security-audit-q4.pdf and confirmed as high priority in stand-up, planned after caching.'),\\n\",\n+       \" Task(task='Implement OAuth refresh token handling (Issue #456)', priority='Medium', etr='Not Stated', status_comments='Status: Not Started. Assigned as issue #456, due EOD today and planned after the security patch.'),\\n\",\n+       \" Task(task='Resolve database migration script schema conflicts for users schema', priority='Medium', etr='Blocked - Time Pending', status_comments=\\\"Status: Blocked. Carry-over from yesterday due to schema conflicts. Latest OCR (14:30Z) confirms the migration attempt failed due to persistent schema conflicts, despite @bsmith's earlier commitment to review the script.\\\")]\"\n+      ]\n+     },\n+     \"execution_count\": 18,\n+     \"metadata\": {},\n+     \"output_type\": \"execute_result\"\n+    }\n+   ],\n+   \"source\": [\n+    \"res.tasks\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"id\": \"ba5f8ef0\",\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": []\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \"env\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\",\n+   \"version\": \"3.10.0\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 5\n+}"
            }
        ],
        "parents": []
    },
    {
        "repository": "Rajasimhareddybolla/bms-ai",
        "sha": "9ca982f6acf3fad96634a5e5773a44b1391864b5",
        "message": "Create ALPHA-beta.py",
        "author": {
            "name": "raja",
            "email": "69231336+Rajasimhareddybolla@users.noreply.github.com",
            "date": "2025-11-06T07:55:12Z"
        },
        "committer": {
            "name": "GitHub",
            "email": "noreply@github.com",
            "date": "2025-11-06T07:55:12Z"
        },
        "stats": {
            "total": 102,
            "additions": 102,
            "deletions": 0
        },
        "files": [
            {
                "sha": "bd35d67c29f09aa94a02d515e60666898988ef8e",
                "filename": "ALPHA-beta.py",
                "status": "added",
                "additions": 102,
                "deletions": 0,
                "changes": 102,
                "blob_url": "https://github.com/Rajasimhareddybolla/bms-AI/blob/9ca982f6acf3fad96634a5e5773a44b1391864b5/ALPHA-beta.py",
                "raw_url": "https://github.com/Rajasimhareddybolla/bms-AI/raw/9ca982f6acf3fad96634a5e5773a44b1391864b5/ALPHA-beta.py",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/bms-AI/contents/ALPHA-beta.py?ref=9ca982f6acf3fad96634a5e5773a44b1391864b5",
                "patch": "@@ -0,0 +1,102 @@\n+import math\n+\n+PLAYER = 'X'\n+OPPONENT = 'O'\n+\n+def check_winner(board):\n+    win_combos = [\n+        [0,1,2], [3,4,5], [6,7,8], \n+        [0,3,6], [1,4,7], [2,5,8], \n+        [0,4,8], [2,4,6]       \n+    ]\n+    for combo in win_combos:\n+        if board[combo[0]] == board[combo[1]] == board[combo[2]] != ' ':\n+            return board[combo[0]]\n+    return None\n+\n+def is_full(board):\n+    return ' ' not in board\n+\n+def evaluate(board):\n+    winner = check_winner(board)\n+    if winner == PLAYER:\n+        return 10\n+    elif winner == OPPONENT:\n+        return -10\n+    else:\n+        return 0\n+\n+def get_children(board, player):\n+    children = []\n+    for i in range(9):\n+        if board[i] == ' ':\n+            new_board = board[:]\n+            new_board[i] = player\n+            children.append((new_board, i))\n+    return children\n+\n+def alpha_beta(board, depth, alpha, beta, maximizing):\n+    score = evaluate(board)\n+    if depth == 0 or score in [10, -10] or is_full(board):\n+        return score\n+\n+    if maximizing:\n+        max_eval = -math.inf\n+        for child, _ in get_children(board, PLAYER):\n+            eval = alpha_beta(child, depth - 1, alpha, beta, False)\n+            max_eval = max(max_eval, eval)\n+            alpha = max(alpha, eval)\n+            if beta <= alpha:\n+                break\n+        return max_eval\n+    else:\n+        min_eval = math.inf\n+        for child, _ in get_children(board, OPPONENT):\n+            eval = alpha_beta(child, depth - 1, alpha, beta, True)\n+            min_eval = min(min_eval, eval)\n+            beta = min(beta, eval)\n+            if beta <= alpha:\n+                break\n+        return min_eval\n+\n+def find_best_move(board):\n+    best_val = -math.inf\n+    best_move = -1\n+    for child, move in get_children(board, PLAYER):\n+        move_val = alpha_beta(child, depth=5, alpha=-math.inf, beta=math.inf, maximizing=False)\n+        if move_val > best_val:\n+            best_val = move_val\n+            best_move = move\n+    return best_move\n+\n+\n+if __name__ == \"__main__\":\n+    board = [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n+    while not is_full(board) and not check_winner(board):\n+        print(\"\\nCurrent board:\")\n+        print(f\"{board[0]} | {board[1]} | {board[2]}\")\n+        print(f\"{board[3]} | {board[4]} | {board[5]}\")\n+        print(f\"{board[6]} | {board[7]} | {board[8]}\")\n+\n+        move = find_best_move(board)\n+        board[move] = PLAYER\n+        print(f\"\\nAI plays at position {move}\")\n+\n+        if check_winner(board) or is_full(board):\n+            break\n+\n+        user_move = int(input(\"Your move (0-8): \"))\n+        if board[user_move] == ' ':\n+            board[user_move] = OPPONENT\n+        else:\n+            print(\"Invalid move. Try again.\")\n+\n+    print(\"\\nFinal board:\")\n+    print(f\"{board[0]} | {board[1]} | {board[2]}\")\n+    print(f\"{board[3]} | {board[4]} | {board[5]}\")\n+    print(f\"{board[6]} | {board[7]} | {board[8]}\")\n+    winner = check_winner(board)\n+    if winner:\n+        print(f\"Winner: {winner}\")\n+    else:\n+        print(\"It's a draw!\")"
            }
        ],
        "parents": [
            "2977585f30824be362ba528551259f9e799be858"
        ]
    },
    {
        "repository": "Rajasimhareddybolla/bms-ai",
        "sha": "2977585f30824be362ba528551259f9e799be858",
        "message": "Add files via upload",
        "author": {
            "name": "raja",
            "email": "69231336+Rajasimhareddybolla@users.noreply.github.com",
            "date": "2025-11-06T07:54:03Z"
        },
        "committer": {
            "name": "GitHub",
            "email": "noreply@github.com",
            "date": "2025-11-06T07:54:03Z"
        },
        "stats": {
            "total": 529,
            "additions": 529,
            "deletions": 0
        },
        "files": [
            {
                "sha": "0d492804e90ca2e3fac7b6451e25e34c6782c2d7",
                "filename": "cnf.py",
                "status": "added",
                "additions": 358,
                "deletions": 0,
                "changes": 358,
                "blob_url": "https://github.com/Rajasimhareddybolla/bms-AI/blob/2977585f30824be362ba528551259f9e799be858/cnf.py",
                "raw_url": "https://github.com/Rajasimhareddybolla/bms-AI/raw/2977585f30824be362ba528551259f9e799be858/cnf.py",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/bms-AI/contents/cnf.py?ref=2977585f30824be362ba528551259f9e799be858",
                "patch": "@@ -0,0 +1,358 @@\n+# CNF converter that **constructs** CNF programmatically (returns structured clauses)\r\n+# Supports: Var, Const, Func, Pred, Not, And, Or, Forall, Exists\r\n+# Implements: eliminate implications, move NOT inwards (NNF), standardize apart, prenex, skolemize,\r\n+#            drop universals, distribute OR over AND to get CNF, and output as list of clauses.\r\n+#\r\n+# This is a compact, general-purpose converter suitable for classroom examples.\r\n+# It will convert the specific formula:\r\n+#   \u2200x [ \u00ac\u2200y \u00ac( Animal(y) \u2228 Loves(x,y) ) \u2228 \u2203y Loves(y,x) ]\r\n+# into CNF clauses (structured), not just print steps.\r\n+#\r\n+# Note: This is educational code \u2014 it handles the common patterns needed for homework problems.\r\n+# It does not implement a full FOL parser; we build the AST directly in Python below.\r\n+\r\n+from copy import deepcopy\r\n+import itertools, uuid\r\n+\r\n+# --- AST node classes ---\r\n+class Var:\r\n+    def __init__(self, name): self.name = name\r\n+    def __repr__(self): return self.name\r\n+\r\n+class Const:\r\n+    def __init__(self, name): self.name = name\r\n+    def __repr__(self): return self.name\r\n+\r\n+class Func:\r\n+    def __init__(self, name, args): self.name = name; self.args = args\r\n+    def __repr__(self): return f\"{self.name}({', '.join(map(str,self.args))})\"\r\n+\r\n+class Pred:\r\n+    def __init__(self, name, args): self.name = name; self.args = args\r\n+    def __repr__(self): return f\"{self.name}({', '.join(map(str,self.args))})\"\r\n+\r\n+class Not:\r\n+    def __init__(self, p): self.p = p\r\n+    def __repr__(self): return f\"\u00ac{self.p}\"\r\n+\r\n+class And:\r\n+    def __init__(self, *conj): self.conj = list(conj)\r\n+    def __repr__(self): return \"(\" + \" \u2227 \".join(map(str,self.conj)) + \")\"\r\n+\r\n+class Or:\r\n+    def __init__(self, *disj): self.disj = list(disj)\r\n+    def __repr__(self): return \"(\" + \" \u2228 \".join(map(str,self.disj)) + \")\"\r\n+\r\n+class Forall:\r\n+    def __init__(self, var, body): self.var = var; self.body = body\r\n+    def __repr__(self): return f\"\u2200{self.var}.{self.body}\"\r\n+\r\n+class Exists:\r\n+    def __init__(self, var, body): self.var = var; self.body = body\r\n+    def __repr__(self): return f\"\u2203{self.var}.{self.body}\"\r\n+\r\n+\r\n+# --- Helper utilities ---\r\n+def fresh_var(prefix=\"v\"):\r\n+    return Var(prefix + \"_\" + uuid.uuid4().hex[:6])\r\n+\r\n+def fresh_skolem_func_name(prefix=\"f\"):\r\n+    return prefix + \"_\" + uuid.uuid4().hex[:6]\r\n+\r\n+def substitute(expr, subs):\r\n+    # subs: mapping of Var.name -> term (Var/Const/Func)\r\n+    if isinstance(expr, Var):\r\n+        return subs.get(expr.name, expr)\r\n+    if isinstance(expr, Const):\r\n+        return expr\r\n+    if isinstance(expr, Func):\r\n+        return Func(expr.name, [substitute(a, subs) for a in expr.args])\r\n+    if isinstance(expr, Pred):\r\n+        return Pred(expr.name, [substitute(a, subs) for a in expr.args])\r\n+    if isinstance(expr, Not):\r\n+        return Not(substitute(expr.p, subs))\r\n+    if isinstance(expr, And):\r\n+        return And(*[substitute(c, subs) for c in expr.conj])\r\n+    if isinstance(expr, Or):\r\n+        return Or(*[substitute(d, subs) for d in expr.disj])\r\n+    if isinstance(expr, Forall):\r\n+        return Forall(expr.var, substitute(expr.body, subs))\r\n+    if isinstance(expr, Exists):\r\n+        return Exists(expr.var, substitute(expr.body, subs))\r\n+    return expr\r\n+\r\n+def free_vars(expr):\r\n+    # collect free variable names in expr\r\n+    if isinstance(expr, Var):\r\n+        return {expr.name}\r\n+    if isinstance(expr, Const):\r\n+        return set()\r\n+    if isinstance(expr, Func):\r\n+        s = set()\r\n+        for a in expr.args: s |= free_vars(a)\r\n+        return s\r\n+    if isinstance(expr, Pred):\r\n+        s = set()\r\n+        for a in expr.args: s |= free_vars(a)\r\n+        return s\r\n+    if isinstance(expr, Not):\r\n+        return free_vars(expr.p)\r\n+    if isinstance(expr, And):\r\n+        s = set()\r\n+        for c in expr.conj: s |= free_vars(c)\r\n+        return s\r\n+    if isinstance(expr, Or):\r\n+        s = set()\r\n+        for d in expr.disj: s |= free_vars(d)\r\n+        return s\r\n+    if isinstance(expr, Forall) or isinstance(expr, Exists):\r\n+        s = free_vars(expr.body)\r\n+        s.discard(expr.var.name)\r\n+        return s\r\n+    return set()\r\n+\r\n+# --- Step 1: eliminate implications (not needed for our formula but kept for completeness) ---\r\n+def eliminate_implications(expr):\r\n+    # This simple AST doesn't include Imply nodes; return as is.\r\n+    return expr\r\n+\r\n+# --- Step 2: push NOT inwards to obtain NNF ---\r\n+def to_nnf(expr):\r\n+    if isinstance(expr, Not):\r\n+        p = expr.p\r\n+        if isinstance(p, Not):\r\n+            return to_nnf(p.p)\r\n+        if isinstance(p, And):\r\n+            return Or(*[to_nnf(Not(c)) for c in p.conj])\r\n+        if isinstance(p, Or):\r\n+            return And(*[to_nnf(Not(d)) for d in p.disj])\r\n+        if isinstance(p, Forall):\r\n+            # \u00ac\u2200x P  => \u2203x \u00acP\r\n+            return Exists(p.var, to_nnf(Not(p.body)))\r\n+        if isinstance(p, Exists):\r\n+            # \u00ac\u2203x P => \u2200x \u00acP\r\n+            return Forall(p.var, to_nnf(Not(p.body)))\r\n+        # predicate or atomic\r\n+        return Not(to_nnf(p))\r\n+    if isinstance(expr, And):\r\n+        return And(*[to_nnf(c) for c in expr.conj])\r\n+    if isinstance(expr, Or):\r\n+        return Or(*[to_nnf(d) for d in expr.disj])\r\n+    if isinstance(expr, Forall):\r\n+        return Forall(expr.var, to_nnf(expr.body))\r\n+    if isinstance(expr, Exists):\r\n+        return Exists(expr.var, to_nnf(expr.body))\r\n+    # atomic\r\n+    return expr\r\n+\r\n+# --- Step 3: standardize apart (rename bound vars to unique names) ---\r\n+def standardize_apart(expr, mapping=None):\r\n+    if mapping is None: mapping = {}\r\n+    if isinstance(expr, Var):\r\n+        return Var(mapping.get(expr.name, expr.name))\r\n+    if isinstance(expr, Const):\r\n+        return expr\r\n+    if isinstance(expr, Func):\r\n+        return Func(expr.name, [standardize_apart(a, mapping) for a in expr.args])\r\n+    if isinstance(expr, Pred):\r\n+        return Pred(expr.name, [standardize_apart(a, mapping) for a in expr.args])\r\n+    if isinstance(expr, Not):\r\n+        return Not(standardize_apart(expr.p, mapping))\r\n+    if isinstance(expr, And):\r\n+        return And(*[standardize_apart(c, mapping) for c in expr.conj])\r\n+    if isinstance(expr, Or):\r\n+        return Or(*[standardize_apart(d, mapping) for d in expr.disj])\r\n+    if isinstance(expr, Forall) or isinstance(expr, Exists):\r\n+        old = expr.var.name\r\n+        new = old + \"_\" + uuid.uuid4().hex[:6]\r\n+        mapping2 = dict(mapping)\r\n+        mapping2[old] = new\r\n+        if isinstance(expr, Forall):\r\n+            return Forall(Var(new), standardize_apart(expr.body, mapping2))\r\n+        else:\r\n+            return Exists(Var(new), standardize_apart(expr.body, mapping2))\r\n+    return expr\r\n+\r\n+# --- Step 4: move quantifiers to prenex normal form (pull quantifiers to top) ---\r\n+def to_prenex(expr):\r\n+    # We assume expr is in NNF\r\n+    if isinstance(expr, Forall):\r\n+        body = to_prenex(expr.body)\r\n+        if isinstance(body, Forall) or isinstance(body, Exists):\r\n+            return type(body)(body.var, to_prenex(Forall(expr.var, body.body)))  # slightly hacky\r\n+        return Forall(expr.var, body)\r\n+    if isinstance(expr, Exists):\r\n+        body = to_prenex(expr.body)\r\n+        if isinstance(body, Forall) or isinstance(body, Exists):\r\n+            return type(body)(body.var, to_prenex(Exists(expr.var, body.body)))\r\n+        return Exists(expr.var, body)\r\n+    if isinstance(expr, And):\r\n+        left = to_prenex(expr.conj[0]) if len(expr.conj) > 0 else None\r\n+        rest = And(*expr.conj[1:]) if len(expr.conj) > 1 else None\r\n+        if rest is None:\r\n+            return to_prenex(expr.conj[0])\r\n+        # combine left and rest recursively\r\n+        L = to_prenex(left); R = to_prenex(rest)\r\n+        # bring quantifiers from L and R to top\r\n+        quantifiers = []\r\n+        while isinstance(L, Forall) or isinstance(L, Exists):\r\n+            quantifiers.append(L)\r\n+            L = L.body\r\n+        while isinstance(R, Forall) or isinstance(R, Exists):\r\n+            quantifiers.append(R)\r\n+            R = R.body\r\n+        core = And(L, R)\r\n+        for q in reversed(quantifiers):\r\n+            core = type(q)(q.var, core)\r\n+        return core\r\n+    if isinstance(expr, Or):\r\n+        # similar treatment as And\r\n+        left = to_prenex(expr.disj[0]) if len(expr.disj) > 0 else None\r\n+        rest = Or(*expr.disj[1:]) if len(expr.disj) > 1 else None\r\n+        if rest is None:\r\n+            return to_prenex(expr.disj[0])\r\n+        L = to_prenex(left); R = to_prenex(rest)\r\n+        quantifiers = []\r\n+        while isinstance(L, Forall) or isinstance(L, Exists):\r\n+            quantifiers.append(L); L = L.body\r\n+        while isinstance(R, Forall) or isinstance(R, Exists):\r\n+            quantifiers.append(R); R = R.body\r\n+        core = Or(L, R)\r\n+        for q in reversed(quantifiers):\r\n+            core = type(q)(q.var, core)\r\n+        return core\r\n+    # atomic or Not\r\n+    return expr\r\n+\r\n+# --- Step 5: Skolemize (remove existentials) ---\r\n+def skolemize(expr, universal_context=None):\r\n+    if universal_context is None:\r\n+        universal_context = []\r\n+    if isinstance(expr, Forall):\r\n+        return Forall(expr.var, skolemize(expr.body, universal_context + [expr.var.name]))\r\n+    if isinstance(expr, Exists):\r\n+        # replace the existential var with a Skolem function of current universals\r\n+        sk_name = fresh_skolem_func_name(\"sk\")\r\n+        args = [Var(v) for v in universal_context]\r\n+        sk_term = Func(sk_name, args) if args else Const(sk_name)\r\n+        subs = {expr.var.name: sk_term}\r\n+        body_sub = substitute(expr.body, subs)\r\n+        return skolemize(body_sub, universal_context)\r\n+    if isinstance(expr, And):\r\n+        return And(*[skolemize(c, universal_context) for c in expr.conj])\r\n+    if isinstance(expr, Or):\r\n+        return Or(*[skolemize(d, universal_context) for d in expr.disj])\r\n+    if isinstance(expr, Not):\r\n+        return Not(skolemize(expr.p, universal_context))\r\n+    return expr\r\n+\r\n+# --- Step 6: drop universal quantifiers (they become implicit) ---\r\n+def drop_universal(expr):\r\n+    if isinstance(expr, Forall):\r\n+        return drop_universal(expr.body)\r\n+    if isinstance(expr, And):\r\n+        return And(*[drop_universal(c) for c in expr.conj])\r\n+    if isinstance(expr, Or):\r\n+        return Or(*[drop_universal(d) for d in expr.disj])\r\n+    if isinstance(expr, Not):\r\n+        return Not(drop_universal(expr.p))\r\n+    return expr\r\n+\r\n+# --- Step 7: distribute OR over AND to obtain CNF ---\r\n+def distribute_or_over_and(or_node):\r\n+    # or_node is Or of elements that may be And/Or/atomic; we recursively distribute\r\n+    # convert list of disjuncts into CNF by pairwise distribution\r\n+    def dist(a, b):\r\n+        # distribute a \u2228 b\r\n+        if isinstance(a, And):\r\n+            return And(*[dist(c, b) for c in a.conj])\r\n+        if isinstance(b, And):\r\n+            return And(*[dist(a, c) for c in b.conj])\r\n+        # otherwise simple Or\r\n+        # flatten nested Ors\r\n+        items = []\r\n+        if isinstance(a, Or): items += a.disj\r\n+        else: items.append(a)\r\n+        if isinstance(b, Or): items += b.disj\r\n+        else: items.append(b)\r\n+        return Or(*items)\r\n+    items = list(or_node.disj)\r\n+    result = items[0]\r\n+    for itm in items[1:]:\r\n+        result = dist(result, itm)\r\n+    return result\r\n+\r\n+def to_cnf_matrix(expr):\r\n+    # expr has no quantifiers (dropped) and is in NNF and Skolemized.\r\n+    if isinstance(expr, And):\r\n+        # CNF is conjunction of clauses\r\n+        clauses = []\r\n+        for c in expr.conj:\r\n+            clauses.extend(to_cnf_matrix(c))\r\n+        return clauses\r\n+    if isinstance(expr, Or):\r\n+        distributed = distribute_or_over_and(expr)\r\n+        if isinstance(distributed, And):\r\n+            # result is And of Ors -> multiple clauses\r\n+            clauses = []\r\n+            for c in distributed.conj:\r\n+                clauses.extend(to_cnf_matrix(c))\r\n+            return clauses\r\n+        # now it's an Or of atoms/negatoms -> single clause\r\n+        lits = []\r\n+        def collect(node):\r\n+            if isinstance(node, Or):\r\n+                for d in node.disj: collect(d)\r\n+            elif isinstance(node, Not):\r\n+                lits.append((\"\u00ac\", node.p))\r\n+            else:\r\n+                lits.append((\"\", node))\r\n+        collect(distributed)\r\n+        return [lits]\r\n+    # atomic or Not\r\n+    if isinstance(expr, Not):\r\n+        return [[(\"\u00ac\", expr.p)]]\r\n+    return [[(\"\", expr)]]\r\n+\r\n+# --- Top-level CNF conversion pipeline ---\r\n+def fol_to_cnf(expr):\r\n+    step0 = eliminate_implications(expr)\r\n+    step1 = to_nnf(step0)\r\n+    step2 = standardize_apart(step1)\r\n+    step3 = to_prenex(step2)\r\n+    step4 = skolemize(step3)\r\n+    step5 = drop_universal(step4)\r\n+    # final: get CNF clauses as list of clauses, each clause is list of (sign, atom)\r\n+    clauses = to_cnf_matrix(step5)\r\n+    return clauses, step0, step1, step2, step3, step4, step5\r\n+\r\n+# --- Build the specific formula AST ---\r\n+# Formula: \u2200x [ \u00ac\u2200y \u00ac( Animal(y) \u2228 Loves(x,y) ) \u2228 \u2203y Loves(y,x) ]\r\n+\r\n+x = Var(\"x\"); y = Var(\"y\")\r\n+animal_y = Pred(\"Animal\", [y])\r\n+loves_xy = Pred(\"Loves\", [x, y])\r\n+inner_or = Or(animal_y, loves_xy)\r\n+neg_inner = Not(inner_or)\r\n+forall_y_neg = Forall(y, neg_inner)               # \u2200y \u00ac(Animal(y) \u2228 Loves(x,y))\r\n+not_forall_y_neg = Not(forall_y_neg)              # \u00ac\u2200y \u00ac(...)\r\n+exists_y_loves_yx = Exists(Var(\"y\"), Pred(\"Loves\", [Var(\"y\"), x]))\r\n+outer_or = Or(not_forall_y_neg, exists_y_loves_yx)\r\n+formula = Forall(x, outer_or)\r\n+\r\n+# Convert to CNF\r\n+clauses, *steps = fol_to_cnf(formula)\r\n+\r\n+print(\"Structured CNF clauses (each clause is a list of (sign, atom)):\\n\")\r\n+for i,cl in enumerate(clauses,1):\r\n+    pretty = []\r\n+    for sign, atom in cl:\r\n+        s = \"~\" if sign==\"\u00ac\" else \"\"\r\n+        pretty.append(s + str(atom))\r\n+    print(f\"Clause {i}: {{ \" + \" \u2228 \".join(pretty) + \" }}\")\r\n+\r\n+print(\"\\nInternal final Skolemized formula (no quantifiers):\")\r\n+print(steps[-1])\r\n+\r\n+# Return the clauses as structured Python data for further programmatic use\r\n+clauses  # this is the final returned value from this cell (visible in the notebook)\r"
            },
            {
                "sha": "b5e2165d4ecfbce286c1d465c2ea9c3d35390d32",
                "filename": "frwd_reasoning.py",
                "status": "added",
                "additions": 171,
                "deletions": 0,
                "changes": 171,
                "blob_url": "https://github.com/Rajasimhareddybolla/bms-AI/blob/2977585f30824be362ba528551259f9e799be858/frwd_reasoning.py",
                "raw_url": "https://github.com/Rajasimhareddybolla/bms-AI/raw/2977585f30824be362ba528551259f9e799be858/frwd_reasoning.py",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/bms-AI/contents/frwd_reasoning.py?ref=2977585f30824be362ba528551259f9e799be858",
                "patch": "@@ -0,0 +1,171 @@\n+\r\n+\r\n+from collections import deque, namedtuple\r\n+\r\n+\r\n+def is_variable(x):\r\n+    return isinstance(x, str) and x and x[0].islower()\r\n+\r\n+def atom_str(atom):\r\n+    pred, args = atom\r\n+    return f\"{pred}({', '.join(args)})\"\r\n+\r\n+# Substitute according to a mapping (var->constant)\r\n+def substitute_atom(atom, subs):\r\n+    pred, args = atom\r\n+    new_args = []\r\n+    for a in args:\r\n+        if is_variable(a) and a in subs:\r\n+            new_args.append(subs[a])\r\n+        else:\r\n+            new_args.append(a)\r\n+    return (pred, tuple(new_args))\r\n+\r\n+# Unification for single terms (simple: variable -> constant only needed here)\r\n+def unify_terms(t1, t2, subs):\r\n+    # t1, t2 are strings; subs is dict var->const\r\n+    t1_val = subs.get(t1, t1) if is_variable(t1) else t1\r\n+    t2_val = subs.get(t2, t2) if is_variable(t2) else t2\r\n+    if t1_val == t2_val:\r\n+        return subs\r\n+    # if t1 is variable, bind it\r\n+    if is_variable(t1):\r\n+        # don't allow binding variable to another variable in this simple scenario; we'll allow var->const only\r\n+        subs2 = dict(subs)\r\n+        subs2[t1] = t2_val\r\n+        return subs2\r\n+    if is_variable(t2):\r\n+        subs2 = dict(subs)\r\n+        subs2[t2] = t1_val\r\n+        return subs2\r\n+    return None\r\n+\r\n+def unify_atoms(a1, a2):\r\n+    # a1,a2 are atoms with same predicate name and same arity\r\n+    p1, args1 = a1\r\n+    p2, args2 = a2\r\n+    if p1 != p2 or len(args1) != len(args2):\r\n+        return None\r\n+    subs = {}\r\n+    for x, y in zip(args1, args2):\r\n+        subs = unify_terms(x, y, subs)\r\n+        if subs is None:\r\n+            return None\r\n+    return subs\r\n+\r\n+# Horn rule: head :- body1, body2, ...\r\n+Rule = namedtuple(\"Rule\", [\"head\", \"body\"])  # head: atom, body: list of atoms (may contain variables)\r\n+\r\n+def forward_chain(rules, facts, query):\r\n+    # rules: list of Rule where variables are lowercase strings like 'x'\r\n+    # facts: set of ground atoms (constants only)\r\n+    # query: atom (ground) to prove\r\n+    inferred = set(facts)\r\n+    agenda = deque(facts)  # facts to be used to try rules\r\n+    proof = {}  # store how each inferred fact was derived: fact -> (rule, substitutions, premises)\r\n+    \r\n+    while agenda:\r\n+        fact = agenda.popleft()\r\n+        # try every rule: for each rule, try to match a body literal with this fact, then check remaining body literals\r\n+        for rule in rules:\r\n+            # For each body literal in the rule, attempt to unify it with the current fact\r\n+            for i, body_lit in enumerate(rule.body):\r\n+                # rename variables in rule to avoid accidental capture (standardize-apart)\r\n+                # We'll append a unique suffix for this attempt\r\n+                suffix = f\"__{id(fact)}_{i}\"\r\n+                def rename_atom(atom):\r\n+                    pred, args = atom\r\n+                    new_args = []\r\n+                    for a in args:\r\n+                        if is_variable(a):\r\n+                            new_args.append(a + suffix)\r\n+                        else:\r\n+                            new_args.append(a)\r\n+                    return (pred, tuple(new_args))\r\n+                renamed_body = [rename_atom(b) for b in rule.body]\r\n+                renamed_head = rename_atom(rule.head)\r\n+                \r\n+                target = renamed_body[i]\r\n+                subs = unify_atoms(target, fact)\r\n+                if subs is None:\r\n+                    continue\r\n+                # Now we have a substitution for one body literal; check other body literals\r\n+                all_ok = True\r\n+                premises = [fact]  # start with matched fact\r\n+                # For each other literal, try to find a matching ground fact in inferred\r\n+                for j, other in enumerate(renamed_body):\r\n+                    if j == i:\r\n+                        continue\r\n+                    # we need to find some ground fact in inferred that unifies with 'other' under subs\r\n+                    matched = False\r\n+                    for candidate in inferred:\r\n+                        s2 = unify_atoms(other, candidate)\r\n+                        if s2 is None:\r\n+                            continue\r\n+                        # merge s2 with subs (simple merge; prefer earlier bindings)\r\n+                        merged = dict(subs)\r\n+                        conflict = False\r\n+                        for k,v in s2.items():\r\n+                            if k in merged and merged[k] != v:\r\n+                                conflict = True; break\r\n+                            merged[k] = v\r\n+                        if conflict:\r\n+                            continue\r\n+                        subs = merged\r\n+                        premises.append(candidate)\r\n+                        matched = True\r\n+                        break\r\n+                    if not matched:\r\n+                        all_ok = False\r\n+                        break\r\n+                if not all_ok:\r\n+                    continue\r\n+                # we can fire the rule with substitution 'subs' to produce head\r\n+                new_head = substitute_atom(renamed_head, subs)\r\n+                if new_head not in inferred:\r\n+                    inferred.add(new_head)\r\n+                    agenda.append(new_head)\r\n+                    proof[new_head] = (rule, subs, tuple(premises))\r\n+                    # check query\r\n+                    if new_head == query:\r\n+                        return True, inferred, proof\r\n+    return (query in inferred), inferred, proof\r\n+\r\n+# Build KB for Marcus\r\n+facts = {\r\n+    (\"Man\", (\"Marcus\",)),\r\n+    (\"Pompeian\", (\"Marcus\",))\r\n+}\r\n+\r\n+rules = [\r\n+    Rule(head=(\"Roman\", (\"x\",)), body=[(\"Pompeian\", (\"x\",))]),\r\n+    Rule(head=(\"Loyal\", (\"x\",)), body=[(\"Roman\", (\"x\",))]),\r\n+    Rule(head=(\"Person\", (\"x\",)), body=[(\"Man\", (\"x\",))]),\r\n+    Rule(head=(\"Mortal\", (\"x\",)), body=[(\"Person\", (\"x\",))])\r\n+]\r\n+\r\n+query = (\"Mortal\", (\"Marcus\",))\r\n+\r\n+proved, inferred, proof = forward_chain(rules, facts, query)\r\n+\r\n+print(\"Forward chaining proof result for query Mortal(Marcus):\", proved)\r\n+print(\"\\nAll inferred facts:\")\r\n+for f in sorted(inferred):\r\n+    print(\" -\", atom_str(f))\r\n+\r\n+if proved:\r\n+    print(\"\\nProof trace (derived facts and how):\")\r\n+    # walk backwards from query using the proof dict\r\n+    def show_derivation(fact, depth=0):\r\n+        indent = \"  \" * depth\r\n+        if fact not in proof:\r\n+            print(indent + f\"{atom_str(fact)}  (given)\")\r\n+            return\r\n+        rule_used, subs_used, premises = proof[fact]\r\n+        head = rule_used.head\r\n+        print(indent + f\"{atom_str(fact)}  (derived via {atom_str(head)} :- {', '.join(atom_str(b) for b in rule_used.body)})\")\r\n+        print(indent + f\"  substitution: {subs_used}\")\r\n+        for p in premises:\r\n+            show_derivation(p, depth+1)\r\n+    show_derivation(query)\r\n+\r"
            }
        ],
        "parents": [
            "41af2a27e12ad4209e57127406095b3ba8fc3e1c"
        ]
    },
    {
        "repository": "Rajasimhareddybolla/bms-ai",
        "sha": "41af2a27e12ad4209e57127406095b3ba8fc3e1c",
        "message": "Update README.md",
        "author": {
            "name": "raja",
            "email": "69231336+Rajasimhareddybolla@users.noreply.github.com",
            "date": "2025-11-06T07:51:02Z"
        },
        "committer": {
            "name": "GitHub",
            "email": "noreply@github.com",
            "date": "2025-11-06T07:51:02Z"
        },
        "stats": {
            "total": 2,
            "additions": 1,
            "deletions": 1
        },
        "files": [
            {
                "sha": "1b98d5baf82b32cd81051c3805d54ed5e97d7236",
                "filename": "README.md",
                "status": "modified",
                "additions": 1,
                "deletions": 1,
                "changes": 2,
                "blob_url": "https://github.com/Rajasimhareddybolla/bms-AI/blob/41af2a27e12ad4209e57127406095b3ba8fc3e1c/README.md",
                "raw_url": "https://github.com/Rajasimhareddybolla/bms-AI/raw/41af2a27e12ad4209e57127406095b3ba8fc3e1c/README.md",
                "contents_url": "https://api.github.com/repos/Rajasimhareddybolla/bms-AI/contents/README.md?ref=41af2a27e12ad4209e57127406095b3ba8fc3e1c",
                "patch": "@@ -1 +1 @@\n-# 1BM23CS074-AI-LAB\n\\ No newline at end of file\n+# 1BM23CS070-AI-LAB"
            }
        ],
        "parents": [
            "207fe947ec03fdb85679f6c1a53ecc1477ac5db8"
        ]
    }
]